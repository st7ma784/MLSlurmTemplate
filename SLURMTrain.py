

from test_tube import SlurmCluster
from test_tube import HyperOptArgumentParser
#from trainclip_v2 import train as train_clip
import os
from trainclip_v37_einsumimp import wandbtrain as train
from demoparser import parser
#For Bede, we need to overwrite the run function in SlurmCluster to remove 'srun' from the start. 
#it appears to improve training because pytorch doesnt get confused by the mulitple CPUs present. 

def __build_slurm_command(self, trial, slurm_cmd_script_path, timestamp, exp_i, on_gpu):

    job_with_version = '{}v{}'.format(self.job_display_name, exp_i)

    sub_commands =['#!/bin/bash',
        '# Auto-generated by test-tube (https://github.com/williamFalcon/test-tube)',   
        '#SBATCH --time={}'.format(self.job_time),
        '#SBATCH --job-name={}'.format(job_with_version),
        '#SBATCH --nodes={}'.format(self.per_experiment_nb_nodes),
        '#SBATCH --mem={}'.format(self.memory_mb_per_node),
        f'#SBATCH --signal=USR1@{self.minutes_to_checkpoint_before_walltime * 60}',
    ]
    # add job name
    # add out output
    if self.enable_log_out:
        sub_commands.append( '#SBATCH --output={}'.format( os.path.join(self.out_log_path, '{}_slurm_output_%j.out'.format(timestamp))))
    # add err output
    if self.enable_log_err:
        sub_commands.append('#SBATCH --error={}'.format(os.path.join(self.err_log_path, '{}_slurm_output_%j.err'.format(timestamp))))

    # add nb of gpus
    if self.per_experiment_nb_gpus > 0 and on_gpu:
        command = '#SBATCH --gres=gpu:{}'.format(self.per_experiment_nb_gpus)
        if self.gpu_type is not None:
            command ='#SBATCH --gres=gpu:{}:{}'.format(self.gpu_type, self.per_experiment_nb_gpus),    
        sub_commands.append(command)

    # add nb of cpus if not looking at a gpu job
    if self.per_experiment_nb_cpus > 0:
        command = '#SBATCH --cpus-per-task={}'.format(self.per_experiment_nb_cpus),
        sub_commands.append(command)

    # Subscribe to email if requested
    mail_type = []
    if self.notify_on_end:
        mail_type.append('END')
    if self.notify_on_fail:
        mail_type.append('FAIL')
    if len(mail_type) > 0:
        sub_commands.append('#SBATCH --mail-type={}'.format(','.join(mail_type)))
        sub_commands.append('#SBATCH --mail-user={}'.format(self.email))

    sub_commands.extend(['#SBATCH --{}={}\n'.format(cmd, value) for  (cmd, value, comment) in self.slurm_commands])
    sub_commands.extend(['module load {}'.format(module) for module in self.modules])
    # remove spaces before the hash
    sub_commands = [x.lstrip() for x in sub_commands]

    # add additional commands
    sub_commands.extend(self.commands)
    # add run command
    trial_args = self.__get_hopt_params(trial)
    trial_args = '{} --{} {} --{} {}'.format(trial_args,
                                                HyperOptArgumentParser.SLURM_CMD_PATH,
                                                slurm_cmd_script_path,
                                                HyperOptArgumentParser.SLURM_EXP_CMD,
                                                exp_i)
    sub_commands.append('{} {} {}'.format(self.python_cmd, self.script_name, trial_args))
    # build full command with empty lines in between
    full_command = '\n'.join(sub_commands)
    print("RUNNING ")
    return full_command


if __name__ == '__main__':
    #When called... 
    argsparser = parser(strategy='random_search')
    hyperparams = argsparser.parse_args()
    # Enable cluster training.
    cluster = SlurmCluster(
        hyperparam_optimizer=hyperparams,
        log_path="/nobackup/projects/$PROJECT/$USER/logs/",#REPLAC $PROJECT and/or $USER
        python_cmd='python3',
        #        test_tube_exp_name="PL_test"
    )
    #Lets manually overwrite and put our method onto the cluster
    cluster.__build_slurm_command = __build_slurm_command
    # Email results if your hpc supports it.
    cluster.notify_job_status(
        email='your@email.com', on_done=True, on_fail=True)

    # SLURM Module to load.  
    # cluster.load_modules([
    #     'python-3',
    #     'anaconda3'
    # ])

    # Add commands to the non-SLURM portion.
    
    cluster.add_command('export CONDADIR=/nobackup/projects/bdlan05/$USER') # We'll assume that on the BEDE/HEC cluster you've named you conda env after the standard...
    cluster.add_command('export wandb=9cf7e97e2460c18a89429deed624ec1cbfb537bc') # 
    cluster.add_command('source $CONDADIR/miniconda3/etc/profile.d/conda.sh') # ...conda setup script
    cluster.add_command('conda activate $CONDADIR/miniconda/envs/open-ce') # ...and activate the conda environment 

    cluster.add_slurm_cmd(
        cmd='account', value='bdlan05', comment='Project account for Bede')
    cluster.add_slurm_cmd(
        cmd='partition', value='gpu', comment='request gpu partition on Bede')
    cluster.cpus_per_task=0 #Bede automatically assigns CPUs if on the GPU partition. 
    cluster.per_experiment_nb_gpus = 1
    cluster.per_experiment_nb_nodes = 1
    #cluster.gpu_type = '1080ti'

    # set a walltime of 24 hours,0, minues
    cluster.job_time = '24:00:00'
    # This triggers a signal n minutes before timout. If you use a different launch option below, this may be important if your code takes a while to checkpoint. 
    cluster.minutes_to_checkpoint_before_walltime = 1
  
    # run the models on the cluster
    cluster.optimize_parallel_cluster_gpu(train, nb_trials=10, job_name='fourth_wandb_trial_batch') # Change this to optimize_parralel_cluster_cpu to debug.
